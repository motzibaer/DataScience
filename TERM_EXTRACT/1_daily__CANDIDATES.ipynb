{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = red> __daily 1:   \"CANDIDATES\"__  </font> #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = grey>1/2: Eingabe von ZIELSPRACHE und AUFTRAGSNUMMER</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "target_language = input(\"Was ist die Zielsprache?\\n en_us  ||  en_uk  ||  fr  ||  pl  ||  es  ||  nl  ||\"\n",
    "                        \"  ru  ||  it  ||  da  ||  bg  ||  cz  ||  hu  ||  pt  ||  ro  || sonst \\n\\n\")\n",
    "\n",
    "### TODO: Reject input in case of incorrect/misspelled input\n",
    "\n",
    "print(\"\\n\")\n",
    "project = input(\"Wie lautet die Projektnummer?\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "### TODO: Order vs. Quote (add examples)\n",
    "### TODO: Reject input in case of incorrect/misspelled input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = grey>2/2: Ausgabe der TERMKANDIDATEN</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\TomWinter\\\\2021\\\\TERM\\\\SKRIPTE\\\\TERM_EXTRAKT\\\\STOPWORD\\\\STOPWORD.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-d69818bfe3a0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0msource\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"SOURCEfile_\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mproject\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m\".txt\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"utf-8\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m \u001B[0mstoppword\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mr\"C:\\Users\\TomWinter\\2021\\TERM\\SKRIPTE\\TERM_EXTRAKT\\STOPWORD\\STOPWORD.csv\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"utf-8\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\TomWinter\\\\2021\\\\TERM\\\\SKRIPTE\\\\TERM_EXTRAKT\\\\STOPWORD\\\\STOPWORD.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "###################################################################\n",
    "#     READ SOURCE FILE(S) AND WRITE TO ONE SINGLE FILE            #\n",
    "###################################################################\n",
    "\n",
    "target = open(\"SOURCE_\" + project+\".txt\", \"w\", encoding='utf-8')\n",
    "\n",
    "for root, dirs, files in os.walk(r\"/Users/mbp/Documents/Google Drive/DataScience/TERM_EXTRACT/files\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"): # Replace with \"ext\" variable with other file format (e.g. csv)?\n",
    "            file = os.path.join(root, file)\n",
    "            file = open(file, \"r\", encoding = \"utf-8\")\n",
    "            for word in sorted(file):\n",
    "                target.write(word + '\\n')\n",
    "target.close()\n",
    "\n",
    "\n",
    "##################################################################### \n",
    "#     CLEAN FILE AND SPLIT TO UNIQUE SINGLE WORDS; WriTE TO FILE    #\n",
    "#####################################################################\n",
    "\n",
    "source = open(\"SOURCE_\" + project+\".txt\", \"r\", encoding='utf-8')             \n",
    "target_2 = open(\"SOURCEfile_\" + project+\".txt\", \"w\", encoding='utf-8')\n",
    "\n",
    "\n",
    "result = []\n",
    "wordstring = source.read()\n",
    "clean_string = (\" \".join(re.findall(r\"[A-Za-züäöÜÄÖß]*\", wordstring))).replace(\"  \",\" \")\n",
    "wordlist = clean_string.split()\n",
    "\n",
    "for word in sorted(wordlist):\n",
    "    if not word in result:\n",
    "        result.append(word)\n",
    "\n",
    "for word in result:\n",
    "    target_2.write('\\n' + word)\n",
    "        \n",
    "source.close()                \n",
    "target_2.close()\n",
    "\n",
    "##################################################################### \n",
    "#                  COMPARISON WITH STOPPWORD                        #\n",
    "#####################################################################\n",
    "\n",
    "source = open(\"SOURCEfile_\" + project+\".txt\", \"r\", encoding = \"utf-8\")\n",
    "stoppword = open(r\"C:\\Users\\TomWinter\\2021\\TERM\\SKRIPTE\\TERM_EXTRAKT\\STOPWORD\\STOPWORD.csv\", \"r\", encoding = \"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "### comparison with stoppword and write to \"Term_Candidates\"\n",
    "list_stoppword=[]\n",
    "for word in stoppword.read().split():    \n",
    "    list_stoppword.append(word)\n",
    "\n",
    "term_cand = []\n",
    "for line in source:\n",
    "    for word in line.lower().split():\n",
    "        if not word in list_stoppword:\n",
    "            term_cand.append(word)\n",
    "\n",
    "source.close()\n",
    "stoppword.close()\n",
    "\n",
    "\n",
    "##################################################################### \n",
    "#                    COMPARISON WITH DB CL                          #\n",
    "#####################################################################\n",
    "target = open(\"Term_Candidates_\" + project+\".txt\", \"a\", encoding = \"utf-8\")\n",
    "df_lookup = pd.read_excel(r\"C:\\Users\\TomWinter\\2021\\TERM\\SKRIPTE\\TERM_EXTRAKT\\LookUP\\lookup.xlsx\", encoding = \"utf-8\")\n",
    "\n",
    "df_low = df_lookup.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "df_lookup_index = df_low.set_index(\"Wert\")\n",
    "\n",
    "#print(df_lookup_index.head())\n",
    "\n",
    "hit = []\n",
    "non_hit = []\n",
    "count = 0\n",
    "count_hit = 0\n",
    "count_non_hit = 0\n",
    "for word in term_cand:\n",
    "    count = count + 1\n",
    "    if word in df_lookup_index.index:\n",
    "        if not word in hit:\n",
    "            hit.append(word)\n",
    "            count_hit = count_hit + 1\n",
    "    else:\n",
    "        non_hit.append(word)\n",
    "        count_non_hit = count_non_hit + 1\n",
    "        \n",
    "for word in non_hit:\n",
    "    target.write(word + \"\\n\")\n",
    "for word in hit:\n",
    "    target.write(\"\\n\\n\\n\" + word + \"\\n\")\n",
    "\n",
    "target.close()    \n",
    "##################################################################### \n",
    "#                               AUSGABE                             #\n",
    "#####################################################################\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Termkandidaten insgesamt:  \" + str(count))\n",
    "print(\"- - - - -\")\n",
    "print(\"\\tdavon unbekannt:     \" + str(count_non_hit))\n",
    "print(\"\\tdavon bereits in CL: \" + str(count_hit))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "#FIRST_SKRIPT___\"SELECTION\"  [DONE]\n",
     "### \n",
     "### 1.  -  INPUT: TARGET LANGUAGE + PROJECT-NUMBER\n",
     "### 2.  -  Read translation file from CSV/TXT and split into words as \"SOURCEfile\"\n",
     "### 3.  -  Compare \"SOURCEfile\" with stopwordlist and LookUp  (!term in ST+TT - comparison with target language)\n",
     "###        + write delta to TXT - \"TERM-CANDIDATES\"\n",
     "\n",
     "    ### Comparison with lookUp missing\n",
     "\n",
     "### 4.  -  --- manual selection --->\n",
     "### 4.1 EXCLUDE NON-TERMS and rename file \"TERMS\"  || 4.2 EXCLUDE TERMS and rename file \"NONTERMS\"\n",
     "#SECOND_SKRIPT___\"FINAL EFFORT\"\n",
     "### 5.  -  UPDATE STOPWORDLIST: \n",
     "###        5.1  compare \"TERMS\" to \"SOURCEfile\" - words not in \"TERMS\": append to stopwordList   \n",
     "###        [5.2  append \"NONTERMS\" to stopwordList + compare \"NONTERMS\" with stopwordlist and LookUp to receive \"TERMS\"]\n",
     "### ---> RESOLUTION: WE STICK TO OPTION 5.1\n",
     "### 6.  -  Compare \"TERMS\" to LookUp (tokenization?) -> GOAL: LIST OF TERMS and estimated effort (\"to verify\" / \"to validate\")\n",
     "### 7.     statistics: amount of terms; amount of term to be verified, amount of terms to be validated\n",
     "### 8. comparison with ahead\n",
     "\n",
     "\n",
     "### TO DO: 5.1 Update Stoppword-List\n",
     "### TO DO: Lemmatisierung\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}